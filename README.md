# Crawler AssÃ­ncrono de CEPs

API REST para processamento assÃ­ncrono de ranges de CEPs utilizando filas e MongoDB.

## âœ… Status da ImplementaÃ§Ã£o

| Requisito                                               | Status |
| ------------------------------------------------------- | ------ |
| POST /cep/crawl (solicitar range)                       | âœ…     |
| GET /cep/crawl/:crawl_id (status)                       | âœ…     |
| GET /cep/crawl/:crawl_id/results (resultados paginados) | âœ…     |
| Processamento assÃ­ncrono via fila                       | âœ…     |
| Controle de taxa (rate limiting)                        | âœ…     |
| Retry com backoff exponencial                           | âœ…     |
| Circuit Breaker para API externa                        | âœ…     |
| Cache de CEPs jÃ¡ consultados                            | âœ…     |
| Swagger/OpenAPI                                         | âœ…     |
| Testes unitÃ¡rios (122 testes)                           | âœ…     |
| Docker Compose completo                                 | âœ…     |

---

## ğŸš€ Como Rodar

### Com Docker (recomendado)

```bash
# 1. Clone e configure
cp .env.example .env

# 2. Suba tudo
docker-compose up --build
```

### Localmente (desenvolvimento)

```bash
# 1. Instale dependÃªncias
pnpm install

# 2. Suba apenas infra (MongoDB + ElasticMQ)
docker-compose up mongo elasticmq -d

# 3. Gere o Prisma Client
pnpm prisma generate && pnpm prisma db push

# 4. Rode API e Worker separadamente
pnpm start:api     # Terminal 1
pnpm start:worker  # Terminal 2
```

### URLs

| ServiÃ§o         | URL                                 |
| --------------- | ----------------------------------- |
| API             | http://localhost:3000               |
| Swagger         | http://localhost:3000/documentation |
| MongoDB Express | http://localhost:8081               |
| ElasticMQ UI    | http://localhost:9325               |

---

## ğŸ“¡ Endpoints

### POST /cep/crawl

Solicita processamento de um range de CEPs.

```bash
curl -X POST http://localhost:3000/cep/crawl \
  -H "Content-Type: application/json" \
  -d '{"cep_start": "01001000", "cep_end": "01001010"}'
```

**Resposta (202 Accepted):**

```json
{
  "crawl_id": "019c2c2f-e62f-7503-9a1d-dc09da92220f",
  "status": "PENDING",
  "total_ceps": 11
}
```

### GET /cep/crawl/:crawl_id

Consulta status do processamento.

```bash
curl http://localhost:3000/cep/crawl/019c2c2f-e62f-7503-9a1d-dc09da92220f
```

**Resposta:**

```json
{
  "crawl_id": "019c2c2f-e62f-7503-9a1d-dc09da92220f",
  "status": "FINISHED",
  "total_ceps": 11,
  "processed_ceps": 11,
  "success_ceps": 8,
  "failed_ceps": 3
}
```

### GET /cep/crawl/:crawl_id/results

Consulta resultados com paginaÃ§Ã£o e filtros.

```bash
# PaginaÃ§Ã£o
curl "http://localhost:3000/cep/crawl/:id/results?page=1&limit=20"

# Filtros
curl "http://localhost:3000/cep/crawl/:id/results?status=SUCCESS&cep_start=01001000"

# Busca por texto (logradouro, bairro, cidade)
curl "http://localhost:3000/cep/crawl/:id/results?q=paulista"
```

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cliente    â”‚â”€â”€â”€â–¶â”‚     API      â”‚â”€â”€â”€â–¶â”‚   MongoDB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ElasticMQ   â”‚  (SQS-compatible)
                    â”‚    (Fila)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Worker     â”‚â”€â”€â”€â–¶â”‚   ViaCEP     â”‚
                    â”‚  (Crawler)   â”‚    â”‚    (API)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Estrutura de Pastas

```
src/
â”œâ”€â”€ controllers/     # Endpoints HTTP (thin layer)
â”œâ”€â”€ handlers/        # LÃ³gica de negÃ³cio dos endpoints
â”œâ”€â”€ services/        # ServiÃ§os de domÃ­nio
â”œâ”€â”€ providers/       # IntegraÃ§Ãµes externas (ViaCEP)
â”œâ”€â”€ repositories/    # Acesso a dados (Prisma)
â”œâ”€â”€ workers/         # Consumidor da fila SQS
â”œâ”€â”€ dtos/            # ValidaÃ§Ã£o de entrada
â”œâ”€â”€ responses/       # Schemas de resposta
â”œâ”€â”€ errors/          # Erros customizados
â””â”€â”€ filters/         # Exception handlers
```

---

## ğŸ¯ DecisÃµes TÃ©cnicas

### 1. Controle de Taxa (Rate Limiting)

O sistema implementa mÃºltiplas camadas de proteÃ§Ã£o contra sobrecarga da API externa:

| Mecanismo    | ConfiguraÃ§Ã£o                | DescriÃ§Ã£o                               |
| ------------ | --------------------------- | --------------------------------------- |
| Rate Limit   | `WORKER_RATE_LIMIT_MS=2000` | Delay entre batches de processamento    |
| ConcorrÃªncia | `WORKER_CONCURRENCY=1`      | CEPs processados em paralelo por worker |
| Timeout      | `VIACEP_TIMEOUT_MS=10000`   | Timeout por requisiÃ§Ã£o                  |
| Retries      | 3 tentativas                | Backoff exponencial: 2s â†’ 4s â†’ 8s       |

### 2. Circuit Breaker

Quando a API externa retorna erros consecutivos, o worker pausa automaticamente:

```
Erro 1 â†’ espera 10s
Erro 2 â†’ espera 20s
Erro 3 â†’ espera 40s
Erro 4+ â†’ espera 60s (mÃ¡ximo)
```

Implementado em `CrawlWorker.startPolling()` com `ThrottlingError` para detectar 429.

### 3. Cache de CEPs

CEPs jÃ¡ consultados sÃ£o armazenados na collection `ceps`, evitando requisiÃ§Ãµes repetidas:

- Se o CEP existe no cache â†’ usa direto, nÃ£o chama ViaCEP
- Reduz drasticamente requisiÃ§Ãµes para ranges sobrepostos

### 4. Recovery de Crawls Incompletos

Se o worker reiniciar, ele detecta crawls nÃ£o finalizados e readiciona apenas os CEPs faltantes na fila:

- Campo `last_recovery_at` evita duplicaÃ§Ã£o de recovery em 10 minutos
- Garante que nenhum CEP seja perdido mesmo com falhas

### 5. SeparaÃ§Ã£o API/Worker

O mesmo cÃ³digo pode rodar como API ou Worker via flag:

```bash
node dist/main --role=api     # Apenas HTTP
node dist/main --role=worker  # Apenas processamento de fila
```

Permite escalar API e Workers independentemente.

---

## âš™ï¸ VariÃ¡veis de Ambiente

| VariÃ¡vel               | DescriÃ§Ã£o               | PadrÃ£o                     |
| ---------------------- | ----------------------- | -------------------------- |
| `API_PORT`             | Porta da API            | `3000`                     |
| `WORKER_REPLICAS`      | InstÃ¢ncias do worker    | `1`                        |
| `WORKER_CONCURRENCY`   | Parallelismo por worker | `1`                        |
| `WORKER_RATE_LIMIT_MS` | Delay entre batches     | `2000`                     |
| `VIACEP_TIMEOUT_MS`    | Timeout ViaCEP          | `10000`                    |
| `VIACEP_URL`           | URL base ViaCEP         | `https://viacep.com.br/ws` |

---

## ğŸ§ª Testes

```bash
# Rodar todos os testes
pnpm test

# Com coverage
pnpm test:cov

# Watch mode
pnpm test:watch
```

**Resultado:** 122 testes passando, cobrindo:

- Controllers, Handlers, Services
- Repositories, Providers
- Workers, DTOs, Validators
- Error handling e edge cases

---

## ğŸ”§ Stack TecnolÃ³gica

| Tecnologia     | Uso                     |
| -------------- | ----------------------- |
| **NestJS**     | Framework principal     |
| **Prisma**     | ORM para MongoDB        |
| **MongoDB**    | Banco de dados          |
| **ElasticMQ**  | Fila compatÃ­vel com SQS |
| **AWS SDK v3** | Cliente SQS             |
| **Axios**      | HTTP client             |
| **Swagger**    | DocumentaÃ§Ã£o da API     |
| **Jest**       | Testes unitÃ¡rios        |
| **Docker**     | ContainerizaÃ§Ã£o         |

---

## ğŸ“ Modelo de Dados

### Collection: `crawls`

```typescript
{
  id: string,              // UUID v7
  cep_start: string,
  cep_end: string,
  status: "PENDING" | "RUNNING" | "FINISHED" | "FAILED",
  total_ceps: number,
  processed_ceps: number,
  success_ceps: number,
  failed_ceps: number,
  last_recovery_at: Date?, // Controle de recovery
  created_at: Date,
  updated_at: Date
}
```

### Collection: `crawl_results`

```typescript
{
  id: string,
  crawl_id: string,
  cep: string,
  status: "SUCCESS" | "ERROR",
  data: JSON?,           // Dados do endereÃ§o
  error_message: string?,
  created_at: Date
}
```

### Collection: `ceps` (cache)

```typescript
{
  cep: string,    // PK
  found: boolean,
  logradouro: string?,
  bairro: string?,
  localidade: string?,
  uf: string?,
  // ... demais campos
}
```

---

## ğŸ“‹ Proposta Original

<details>
<summary>Clique para expandir os requisitos originais</summary>

### A proposta: Crawler assÃ­ncrono de CEPs + Fila + MongoDB

A ideia Ã© bem simples:

- [x] uma API que permita solicitar o processamento de um **range de CEPs**
- [x] cada CEP do range deve ser processado de forma **assÃ­ncrona**
- [x] os dados devem ser obtidos a partir da API pÃºblica do **ViaCEP**
- [x] os resultados e o progresso devem ser persistidos em um banco **MongoDB**

### API

- [x] rota `POST /cep/crawl` que recebe um range de CEPs
- [x] validaÃ§Ã£o de formato, ordem e tamanho mÃ¡ximo do range
- [x] criar identificador Ãºnico (`crawl_id`)
- [x] inserir um item na fila para cada CEP
- [x] retornar `202 Accepted` com o `crawl_id`

- [x] rota `GET /cep/crawl/:crawl_id` com status do processamento
- [x] retornar `404` se nÃ£o existir, `200` se existir

- [x] rota `GET /cep/crawl/:crawl_id/results` com paginaÃ§Ã£o

### Processamento assÃ­ncrono

- [x] processamento fora do ciclo HTTP
- [x] consumo individual da fila
- [x] persistÃªncia no MongoDB
- [x] suporte a retry em falhas temporÃ¡rias

### Fila assÃ­ncrona

- [x] uso do ElasticMQ (compatÃ­vel com SQS)
- [x] controle de taxa para nÃ£o exceder limites da API externa

### PersistÃªncia

- [x] MongoDB com dados associados ao `crawl_id`
- [x] modelo permite acompanhar progresso e identificar erros

### Infraestrutura

- [x] Dockerfile para a aplicaÃ§Ã£o
- [x] docker-compose.yml com API, Worker, MongoDB e ElasticMQ

</details>

---

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido como teste tÃ©cnico para posiÃ§Ã£o de Backend Developer.
